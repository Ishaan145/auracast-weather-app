{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bj7Kxinb5Zka"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Available NASA POWER Parameters:\n",
        "features = {\n",
        "    'T2M': 'Temperature at 2 Meters (°C)',\n",
        "    'T2M_MIN': 'Minimum Temperature',\n",
        "    'T2M_MAX': 'Maximum Temperature',\n",
        "    'PRECTOTCORR': 'Precipitation Corrected (mm/day)',\n",
        "    'WS10M': 'Wind Speed at 10m',\n",
        "    'RH2M': 'Relative Humidity at 2m',\n",
        "    'PS': 'Surface Pressure',\n",
        "    'ALLSKY_SFC_SW_DWN': 'Solar Radiation'\n",
        "}"
      ],
      "metadata": {
        "id": "ojxS1RPbamb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target Variables (Multi-Output Classification):\n",
        "targets = {\n",
        "    'very_hot_risk': 'Binary (1 if T2M_MAX > 90th percentile)',\n",
        "    'very_cold_risk': 'Binary (1 if T2M_MIN < 10th percentile)',\n",
        "    'very_windy_risk': 'Binary (1 if WS10M > 90th percentile OR >40mph)',\n",
        "    'very_wet_risk': 'Binary (1 if PRECTOTCORR > 90th percentile OR >76mm)',\n",
        "    'uncomfortable_risk': 'Binary (1 if Heat Index >103°F OR Wind Chill <-20°F)'\n",
        "}"
      ],
      "metadata": {
        "id": "t_V9i894aunO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Historical Probability Model\n",
        "# Simpler, interpretable approach using statistical methods\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def calculate_historical_probabilities(data, date, window_days=7):\n",
        "    \"\"\"\n",
        "    For a given date, calculate probabilities based on\n",
        "    historical frequency within ±window_days\n",
        "    \"\"\"\n",
        "    target_day_of_year = date.timetuple().tm_yday\n",
        "\n",
        "    # Filter data to similar dates across all years\n",
        "    mask = (data.index.dayofyear >= target_day_of_year - window_days) & \\\n",
        "           (data.index.dayofyear <= target_day_of_year + window_days)\n",
        "    historical_data = data[mask]\n",
        "\n",
        "    # Calculate percentile thresholds\n",
        "    thresholds = {\n",
        "        'very_hot': historical_data['T2M_MAX'].quantile(0.90),\n",
        "        'very_cold': historical_data['T2M_MIN'].quantile(0.10),\n",
        "        'very_windy': historical_data['WS10M'].quantile(0.90),\n",
        "        'very_wet': historical_data['PRECTOTCORR'].quantile(0.90)\n",
        "    }\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probabilities = {\n",
        "        'very_hot': (historical_data['T2M_MAX'] > thresholds['very_hot']).mean(),\n",
        "        'very_cold': (historical_data['T2M_MIN'] < thresholds['very_cold']).mean(),\n",
        "        'very_windy': (historical_data['WS10M'] > thresholds['very_windy']).mean(),\n",
        "        'very_wet': (historical_data['PRECTOTCORR'] > thresholds['very_wet']).mean()\n",
        "    }\n",
        "\n",
        "    return probabilities, thresholds"
      ],
      "metadata": {
        "id": "05XLdrVFbgWw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Classification Model"
      ],
      "metadata": {
        "id": "GkhW6T9-kPwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ML Classification Model\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Feature engineering\n",
        "def create_features(df):\n",
        "    df['day_of_year'] = df.index.dayofyear\n",
        "    df['month'] = df.index.month\n",
        "    df['year'] = df.index.year\n",
        "\n",
        "    # Rolling statistics (7-day window)\n",
        "    df['temp_7day_mean'] = df['T2M'].rolling(7).mean()\n",
        "    df['temp_7day_std'] = df['T2M'].rolling(7).std()\n",
        "    df['precip_7day_sum'] = df['PRECTOTCORR'].rolling(7).sum()\n",
        "\n",
        "    # Lag features\n",
        "    df['temp_lag1'] = df['T2M'].shift(1)\n",
        "    df['temp_lag7'] = df['T2M'].shift(7)\n",
        "    df['precip_lag1'] = df['PRECTOTCORR'].shift(1)\n",
        "\n",
        "    # Seasonal indicators\n",
        "    df['is_monsoon'] = ((df['month'] >= 6) & (df['month'] <= 9)).astype(int)\n",
        "    df['is_winter'] = ((df['month'] >= 11) | (df['month'] <= 2)).astype(int)\n",
        "\n",
        "    return df.dropna()\n",
        "\n",
        "# Multi-output classification\n",
        "model = MultiOutputClassifier(\n",
        "    xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        objective='binary:logistic'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "LaThVokykLn2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation"
      ],
      "metadata": {
        "id": "Aar75y4ScpYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation\n",
        "\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NASA POWER API Configuration\n",
        "BASE_URL = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
        "\n",
        "def fetch_nasa_power_data(lat, lon, start_date, end_date, parameters):\n",
        "    \"\"\"Fetch data from NASA POWER API\"\"\"\n",
        "    params = {\n",
        "        'parameters': ','.join(parameters),\n",
        "        'community': 'ag',\n",
        "        'longitude': lon,\n",
        "        'latitude': lat,\n",
        "        'start': start_date.strftime('%Y%m%d'),\n",
        "        'end': end_date.strftime('%Y%m%d'),\n",
        "        'format': 'json'\n",
        "    }\n",
        "\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    return response.json()\n",
        "\n",
        "def process_nasa_data(json_data):\n",
        "    \"\"\"Convert NASA POWER JSON to DataFrame\"\"\"\n",
        "    parameters = json_data['properties']['parameter']\n",
        "\n",
        "    # Create DataFrame from nested dict\n",
        "    df_dict = {}\n",
        "    for param_name, param_data in parameters.items():\n",
        "        df_dict[param_name] = param_data\n",
        "\n",
        "    df = pd.DataFrame(df_dict)\n",
        "    df.index = pd.to_datetime(df.index, format='%Y%m%d')\n",
        "    df = df.replace(-999.0, np.nan)  # Handle fill values\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_adverse_conditions(df, location_specific=True):\n",
        "    \"\"\"Create binary target variables for adverse conditions\"\"\"\n",
        "    targets = pd.DataFrame(index=df.index)\n",
        "\n",
        "    if location_specific:\n",
        "        # Calculate percentile thresholds for each day of year\n",
        "        for day in range(1, 367):\n",
        "            mask = df.index.dayofyear == day\n",
        "            if mask.sum() > 0:\n",
        "                # Very Hot: 90th percentile\n",
        "                threshold_hot = df.loc[mask, 'T2M_MAX'].quantile(0.90)\n",
        "                targets.loc[mask, 'very_hot'] = (df.loc[mask, 'T2M_MAX'] > threshold_hot).astype(int)\n",
        "\n",
        "                # Very Cold: 10th percentile\n",
        "                threshold_cold = df.loc[mask, 'T2M_MIN'].quantile(0.10)\n",
        "                targets.loc[mask, 'very_cold'] = (df.loc[mask, 'T2M_MIN'] < threshold_cold).astype(int)\n",
        "\n",
        "                # Very Windy: 90th percentile\n",
        "                if 'WS10M' in df.columns:\n",
        "                    threshold_wind = df.loc[mask, 'WS10M'].quantile(0.90)\n",
        "                    targets.loc[mask, 'very_windy'] = (df.loc[mask, 'WS10M'] > threshold_wind).astype(int)\n",
        "\n",
        "                # Very Wet: 90th percentile of rainy days\n",
        "                rainy_days = df.loc[mask & (df['PRECTOTCORR'] > 0), 'PRECTOTCORR']\n",
        "                if len(rainy_days) > 0:\n",
        "                    threshold_wet = rainy_days.quantile(0.90)\n",
        "                    targets.loc[mask, 'very_wet'] = (df.loc[mask, 'PRECTOTCORR'] > threshold_wet).astype(int)\n",
        "\n",
        "    # Absolute thresholds (safety-based)\n",
        "    if 'WS10M' in df.columns:\n",
        "        targets['very_windy_absolute'] = (df['WS10M'] >= 64.4).astype(int)  # 40 mph\n",
        "\n",
        "    targets['very_wet_absolute'] = (df['PRECTOTCORR'] >= 76.2).astype(int)  # 3 inches\n",
        "\n",
        "    # Calculate Heat Index (simplified)\n",
        "    if 'RH2M' in df.columns:\n",
        "        T = df['T2M_MAX'] * 9/5 + 32  # Convert to Fahrenheit\n",
        "        R = df['RH2M']\n",
        "        HI = -42.379 + 2.04901523*T + 10.14333127*R - 0.22475541*T*R\n",
        "        targets['uncomfortable_hot'] = (HI > 103).astype(int)\n",
        "\n",
        "    return targets.fillna(0)\n",
        "\n",
        "\n",
        "def create_features(df):\n",
        "    \"\"\"Create predictive features for probability model\"\"\"\n",
        "    features = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Temporal features\n",
        "    features['day_of_year'] = df.index.dayofyear\n",
        "    features['month'] = df.index.month\n",
        "    features['day_sin'] = np.sin(2 * np.pi * features['day_of_year'] / 365)\n",
        "    features['day_cos'] = np.cos(2 * np.pi * features['day_of_year'] / 365)\n",
        "\n",
        "    # Rolling statistics (7-day and 30-day)\n",
        "    for window in [7, 30]:\n",
        "        features[f'temp_mean_{window}d'] = df['T2M'].rolling(window).mean()\n",
        "        features[f'temp_std_{window}d'] = df['T2M'].rolling(window).std()\n",
        "        features[f'precip_sum_{window}d'] = df['PRECTOTCORR'].rolling(window).sum()\n",
        "        features[f'precip_days_{window}d'] = (df['PRECTOTCORR'] > 0).rolling(window).sum()\n",
        "\n",
        "    # Lag features\n",
        "    for lag in [1, 7, 30]:\n",
        "        features[f'temp_lag{lag}'] = df['T2M'].shift(lag)\n",
        "        features[f'precip_lag{lag}'] = df['PRECTOTCORR'].shift(lag)\n",
        "\n",
        "    # Climatological anomaly\n",
        "    features['temp_anomaly'] = df['T2M'] - df.groupby(df.index.dayofyear)['T2M'].transform('mean')\n",
        "\n",
        "    return features.dropna()\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    LAT, LON = 28.6, 77.2  # New Delhi\n",
        "    START_DATE = datetime(1990, 1, 1)\n",
        "    END_DATE = datetime(2023, 12, 31)\n",
        "\n",
        "    PARAMETERS = ['T2M', 'T2M_MIN', 'T2M_MAX', 'PRECTOTCORR',\n",
        "                  'WS10M', 'RH2M', 'PS']\n",
        "\n",
        "    print(\"Fetching NASA POWER data...\")\n",
        "    raw_data = fetch_nasa_power_data(LAT, LON, START_DATE, END_DATE, PARAMETERS)\n",
        "\n",
        "    print(\"Processing data...\")\n",
        "    df = process_nasa_data(raw_data)\n",
        "\n",
        "    print(\"Creating target variables...\")\n",
        "    targets = calculate_adverse_conditions(df)\n",
        "\n",
        "    print(\"Engineering features...\")\n",
        "    features_df = create_features(df)\n",
        "\n",
        "    # Align features and targets\n",
        "    common_idx = features_df.index.intersection(targets.index)\n",
        "    X = features_df.loc[common_idx]\n",
        "    y = targets.loc[common_idx]\n",
        "\n",
        "    # Time-based split (important for time series)\n",
        "    split_date = datetime(2020, 1, 1)\n",
        "    X_train = X[X.index < split_date]\n",
        "    X_test = X[X.index >= split_date]\n",
        "    y_train = y[y.index < split_date]\n",
        "    y_test = y[y.index >= split_date]\n",
        "\n",
        "    print(f\"\\nTraining set: {len(X_train)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining multi-output classifier...\")\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "    model = MultiOutputClassifier(\n",
        "        RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, col in enumerate(y.columns):\n",
        "        print(f\"\\n{col.upper()}:\")\n",
        "        # Check if there are any samples for this target in the test set\n",
        "        if y_test.iloc[:, i].sum() == 0 and (1 - y_test.iloc[:, i]).sum() == 0:\n",
        "            print(\"No samples for this target in the test set.\")\n",
        "            continue\n",
        "\n",
        "        # Check unique classes in the test set for the current target\n",
        "        unique_classes = y_test.iloc[:, i].unique()\n",
        "        if len(unique_classes) == 1:\n",
        "            # If only one class is present, adjust target_names and labels\n",
        "            if unique_classes[0] == 0:\n",
        "                print(classification_report(y_test.iloc[:, i], y_pred[:, i],\n",
        "                                           target_names=['Normal'], labels=[0]))\n",
        "            else:\n",
        "                print(classification_report(y_test.iloc[:, i], y_pred[:, i],\n",
        "                                           target_names=['Adverse'], labels=[1]))\n",
        "        else:\n",
        "            # If both classes are present, use default\n",
        "            print(classification_report(y_test.iloc[:, i], y_pred[:, i],\n",
        "                                       target_names=['Normal', 'Adverse']))\n",
        "\n",
        "\n",
        "    # Save model\n",
        "    import joblib\n",
        "    joblib.dump(model, 'auracast_weather_model.pkl')\n",
        "    print(\"\\nModel saved as 'auracast_weather_model.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhkA11V6bh_8",
        "outputId": "36808bd2-e6a4-4243-b5cc-6e545248568a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching NASA POWER data...\n",
            "Processing data...\n",
            "Creating target variables...\n",
            "Engineering features...\n",
            "\n",
            "Training set: 10927 samples\n",
            "Test set: 1461 samples\n",
            "\n",
            "Training multi-output classifier...\n",
            "\n",
            "==================================================\n",
            "MODEL EVALUATION\n",
            "==================================================\n",
            "\n",
            "VERY_HOT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.97      0.98      0.97      1358\n",
            "     Adverse       0.67      0.66      0.67       103\n",
            "\n",
            "    accuracy                           0.95      1461\n",
            "   macro avg       0.82      0.82      0.82      1461\n",
            "weighted avg       0.95      0.95      0.95      1461\n",
            "\n",
            "\n",
            "VERY_COLD:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.93      0.96      0.94      1265\n",
            "     Adverse       0.64      0.51      0.57       196\n",
            "\n",
            "    accuracy                           0.90      1461\n",
            "   macro avg       0.78      0.73      0.75      1461\n",
            "weighted avg       0.89      0.90      0.89      1461\n",
            "\n",
            "\n",
            "VERY_WINDY:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.90      1.00      0.95      1311\n",
            "     Adverse       0.60      0.02      0.04       150\n",
            "\n",
            "    accuracy                           0.90      1461\n",
            "   macro avg       0.75      0.51      0.49      1461\n",
            "weighted avg       0.87      0.90      0.85      1461\n",
            "\n",
            "\n",
            "VERY_WET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.94      1.00      0.97      1366\n",
            "     Adverse       0.50      0.03      0.06        95\n",
            "\n",
            "    accuracy                           0.93      1461\n",
            "   macro avg       0.72      0.51      0.51      1461\n",
            "weighted avg       0.91      0.93      0.91      1461\n",
            "\n",
            "\n",
            "VERY_WINDY_ABSOLUTE:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      1.00      1.00      1461\n",
            "\n",
            "    accuracy                           1.00      1461\n",
            "   macro avg       1.00      1.00      1.00      1461\n",
            "weighted avg       1.00      1.00      1.00      1461\n",
            "\n",
            "\n",
            "VERY_WET_ABSOLUTE:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      1.00      1.00      1461\n",
            "\n",
            "    accuracy                           1.00      1461\n",
            "   macro avg       1.00      1.00      1.00      1461\n",
            "weighted avg       1.00      1.00      1.00      1461\n",
            "\n",
            "\n",
            "UNCOMFORTABLE_HOT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      1.00      1.00      1461\n",
            "\n",
            "    accuracy                           1.00      1461\n",
            "   macro avg       1.00      1.00      1.00      1461\n",
            "weighted avg       1.00      1.00      1.00      1461\n",
            "\n",
            "\n",
            "Model saved as 'auracast_weather_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics"
      ],
      "metadata": {
        "id": "Rq8i1Eq4cT5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrics\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "def evaluate_model(y_true, y_pred_proba, condition_names):\n",
        "    \"\"\"Comprehensive evaluation with business metrics\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for i, name in enumerate(condition_names):\n",
        "        # Precision-Recall curve\n",
        "        precision, recall, thresholds = precision_recall_curve(\n",
        "            y_true[:, i], y_pred_proba[:, i]\n",
        "        )\n",
        "\n",
        "        # Average Precision Score\n",
        "        ap_score = average_precision_score(y_true[:, i], y_pred_proba[:, i])\n",
        "\n",
        "        # ROC-AUC\n",
        "        roc_auc = roc_auc_score(y_true[:, i], y_pred_proba[:, i])\n",
        "\n",
        "        results[name] = {\n",
        "            'AP': ap_score,\n",
        "            'ROC-AUC': roc_auc,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall\n",
        "        }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "0tcReto_b0xo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core Implementation Strategy"
      ],
      "metadata": {
        "id": "Sz-Fxh1McRof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import brier_score_loss\n",
        "import joblib\n",
        "\n",
        "class AuraCastProbabilityModel:\n",
        "    \"\"\"\n",
        "    Climatological probability model based on NASA POWER data\n",
        "    Implements tercile-based probability forecasting with calibration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.calibrator = None\n",
        "        self.percentile_thresholds = {}\n",
        "\n",
        "    def fetch_nasa_power_data(self, lat, lon, start_year, end_year):\n",
        "        \"\"\"Fetch multi-year data from NASA POWER API\"\"\"\n",
        "        BASE_URL = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
        "\n",
        "        start_date = f\"{start_year}0101\"\n",
        "        end_date = f\"{end_year}1231\"\n",
        "\n",
        "        params = {\n",
        "            'parameters': 'T2M,T2M_MAX,T2M_MIN,PRECTOTCORR,WS10M,RH2M,PS',\n",
        "            'community': 'ag',\n",
        "            'longitude': lon,\n",
        "            'latitude': lat,\n",
        "            'start': start_date,\n",
        "            'end': end_date,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            raise Exception(f\"API Error: {response.status_code}\")\n",
        "\n",
        "    def process_nasa_data(self, json_data):\n",
        "        \"\"\"Convert NASA JSON to DataFrame\"\"\"\n",
        "        parameters = json_data['properties']['parameter']\n",
        "\n",
        "        df_dict = {}\n",
        "        for param_name, param_data in parameters.items():\n",
        "            df_dict[param_name] = param_data\n",
        "\n",
        "        df = pd.DataFrame(df_dict)\n",
        "        df.index = pd.to_datetime(df.index, format='%Y%m%d')\n",
        "        df = df.replace(-999.0, np.nan)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def calculate_day_of_year_statistics(self, df, window=15):\n",
        "        \"\"\"\n",
        "        Calculate percentile thresholds for each day of year\n",
        "        Uses ±window days for robust statistics\n",
        "        \"\"\"\n",
        "        stats = {}\n",
        "\n",
        "        for target_day in range(1, 367):\n",
        "            # Create mask for days within window\n",
        "            day_mask = (\n",
        "                (df.index.dayofyear >= target_day - window) &\n",
        "                (df.index.dayofyear <= target_day + window)\n",
        "            )\n",
        "            window_data = df[day_mask]\n",
        "\n",
        "            if len(window_data) < 10:  # Minimum sample size\n",
        "                continue\n",
        "\n",
        "            stats[target_day] = {\n",
        "                'temp_max_90p': window_data['T2M_MAX'].quantile(0.90),\n",
        "                'temp_max_10p': window_data['T2M_MAX'].quantile(0.10),\n",
        "                'temp_min_90p': window_data['T2M_MIN'].quantile(0.90),\n",
        "                'temp_min_10p': window_data['T2M_MIN'].quantile(0.10),\n",
        "                'precip_90p': window_data[window_data['PRECTOTCORR'] > 0]['PRECTOTCORR'].quantile(0.90),\n",
        "                'wind_90p': window_data['WS10M'].quantile(0.90) if 'WS10M' in window_data else None,\n",
        "                'sample_size': len(window_data)\n",
        "            }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def create_tercile_targets(self, df, stats):\n",
        "        \"\"\"\n",
        "        Create tercile-based probability targets\n",
        "        Based on WCS methodology from your documents\n",
        "        \"\"\"\n",
        "        targets = pd.DataFrame(index=df.index)\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            day = idx.dayofyear\n",
        "            if day not in stats:\n",
        "                continue\n",
        "\n",
        "            # Very Hot: Above 90th percentile (upper tercile)\n",
        "            targets.loc[idx, 'very_hot'] = int(row['T2M_MAX'] > stats[day]['temp_max_90p'])\n",
        "\n",
        "            # Very Cold: Below 10th percentile (lower tercile)\n",
        "            targets.loc[idx, 'very_cold'] = int(row['T2M_MIN'] < stats[day]['temp_min_10p'])\n",
        "\n",
        "            # Very Wet: Above 90th percentile of rainy days\n",
        "            if pd.notna(row['PRECTOTCORR']) and row['PRECTOTCORR'] > 0:\n",
        "                targets.loc[idx, 'very_wet'] = int(row['PRECTOTCORR'] > stats[day]['precip_90p'])\n",
        "            else:\n",
        "                targets.loc[idx, 'very_wet'] = 0\n",
        "\n",
        "            # Very Windy: Above 90th percentile OR absolute threshold\n",
        "            if 'WS10M' in row and pd.notna(row['WS10M']):\n",
        "                targets.loc[idx, 'very_windy'] = int(\n",
        "                    (row['WS10M'] > stats[day]['wind_90p']) or (row['WS10M'] >= 17.9)  # 40mph\n",
        "                )\n",
        "\n",
        "        return targets.dropna()\n",
        "\n",
        "    def engineer_features(self, df):\n",
        "        \"\"\"Create predictive features for probability model\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "\n",
        "        # Temporal features\n",
        "        features['day_of_year'] = df.index.dayofyear\n",
        "        features['month'] = df.index.month\n",
        "        features['day_sin'] = np.sin(2 * np.pi * features['day_of_year'] / 365)\n",
        "        features['day_cos'] = np.cos(2 * np.pi * features['day_of_year'] / 365)\n",
        "\n",
        "        # Rolling statistics (7-day and 30-day)\n",
        "        for window in [7, 30]:\n",
        "            features[f'temp_mean_{window}d'] = df['T2M'].rolling(window).mean()\n",
        "            features[f'temp_std_{window}d'] = df['T2M'].rolling(window).std()\n",
        "            features[f'precip_sum_{window}d'] = df['PRECTOTCORR'].rolling(window).sum()\n",
        "            features[f'precip_days_{window}d'] = (df['PRECTOTCORR'] > 0).rolling(window).sum()\n",
        "\n",
        "        # Lag features\n",
        "        for lag in [1, 7, 30]:\n",
        "            features[f'temp_lag{lag}'] = df['T2M'].shift(lag)\n",
        "            features[f'precip_lag{lag}'] = df['PRECTOTCORR'].shift(lag)\n",
        "\n",
        "        # Climatological anomaly\n",
        "        features['temp_anomaly'] = df['T2M'] - df.groupby(df.index.dayofyear)['T2M'].transform('mean')\n",
        "\n",
        "        return features.dropna()\n",
        "\n",
        "    def train_calibrated_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train probability model with calibration\n",
        "        Implements reliability framework from your documents\n",
        "        \"\"\"\n",
        "        # Base classifier\n",
        "        base_model = RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=15,\n",
        "            min_samples_split=50,\n",
        "            min_samples_leaf=20,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # Calibration for reliable probabilities (isotonic regression)\n",
        "        self.model = CalibratedClassifierCV(\n",
        "            base_model,\n",
        "            method='isotonic',\n",
        "            cv=5\n",
        "        )\n",
        "\n",
        "        self.model.fit(X_train, y_train)\n",
        "        return self\n",
        "\n",
        "    def calculate_reliability_metrics(self, y_true, y_pred_proba, n_bins=10):\n",
        "        \"\"\"\n",
        "        Calculate reliability diagram data\n",
        "        Based on WCS reliability assessment methodology\n",
        "        \"\"\"\n",
        "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "\n",
        "        observed_freq = []\n",
        "        forecast_prob = []\n",
        "        bin_counts = []\n",
        "\n",
        "        for i in range(n_bins):\n",
        "            mask = (y_pred_proba >= bin_edges[i]) & (y_pred_proba < bin_edges[i+1])\n",
        "            if mask.sum() > 0:\n",
        "                observed_freq.append(y_true[mask].mean())\n",
        "                forecast_prob.append(bin_centers[i])\n",
        "                bin_counts.append(mask.sum())\n",
        "\n",
        "        return {\n",
        "            'forecast_prob': forecast_prob,\n",
        "            'observed_freq': observed_freq,\n",
        "            'counts': bin_counts,\n",
        "            'brier_score': brier_score_loss(y_true, y_pred_proba)\n",
        "        }\n",
        "\n",
        "    def predict_probabilities(self, X, target_date):\n",
        "        \"\"\"\n",
        "        Predict tercile probabilities for target date\n",
        "        Returns probabilities aligned with your frontend structure\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise Exception(\"Model not trained. Call train_calibrated_model first.\")\n",
        "\n",
        "        # Get probabilities for each adverse condition\n",
        "        probabilities = {}\n",
        "\n",
        "        for condition in ['very_hot', 'very_cold', 'very_windy', 'very_wet']:\n",
        "            prob = self.model.predict_proba(X)[:, 1]  # Probability of adverse condition\n",
        "            probabilities[condition] = {\n",
        "                'probability': float(prob[0]),\n",
        "                'threshold': self.get_threshold_for_date(target_date, condition),\n",
        "                'confidence': self.calculate_confidence_score(X)\n",
        "            }\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def calculate_confidence_score(self, X):\n",
        "        \"\"\"\n",
        "        Calculate statistical confidence based on data quality\n",
        "        Your research mentions this as key trust metric\n",
        "        \"\"\"\n",
        "        # Factors: sample size, data completeness, model uncertainty\n",
        "        base_confidence = 0.85\n",
        "\n",
        "        # Adjust for feature completeness\n",
        "        missing_ratio = X.isna().sum().sum() / X.size\n",
        "        completeness_penalty = missing_ratio * 0.2\n",
        "\n",
        "        return max(0.6, base_confidence - completeness_penalty)\n",
        "\n",
        "# Main execution script\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"AURACAST PROBABILITY MODEL TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Configuration\n",
        "    LAT, LON = 28.6, 77.2  # New Delhi\n",
        "    START_YEAR = 1990\n",
        "    END_YEAR = 2023\n",
        "\n",
        "    # Initialize model\n",
        "    model = AuraCastProbabilityModel()\n",
        "\n",
        "    # Fetch data\n",
        "    print(f\"\\n1. Fetching NASA POWER data ({START_YEAR}-{END_YEAR})...\")\n",
        "    raw_data = model.fetch_nasa_power_data(LAT, LON, START_YEAR, END_YEAR)\n",
        "\n",
        "    # Process data\n",
        "    print(\"2. Processing weather data...\")\n",
        "    df = model.process_nasa_data(raw_data)\n",
        "    print(f\"   Data points: {len(df)}\")\n",
        "\n",
        "    # Calculate climatological statistics\n",
        "    print(\"3. Calculating day-of-year statistics...\")\n",
        "    stats = model.calculate_day_of_year_statistics(df, window=15)\n",
        "    model.percentile_thresholds = stats\n",
        "\n",
        "    # Create targets\n",
        "    print(\"4. Creating tercile-based targets...\")\n",
        "    targets = model.create_tercile_targets(df, stats)\n",
        "\n",
        "    # Engineer features\n",
        "    print(\"5. Engineering predictive features...\")\n",
        "    features = model.engineer_features(df)\n",
        "\n",
        "    # Align data\n",
        "    common_idx = features.index.intersection(targets.index)\n",
        "    X = features.loc[common_idx]\n",
        "    y = targets.loc[common_idx]\n",
        "\n",
        "    # Time-based split\n",
        "    split_date = datetime(2020, 1, 1)\n",
        "    train_mask = X.index < split_date\n",
        "    test_mask = X.index >= split_date\n",
        "\n",
        "    X_train, X_test = X[train_mask], X[test_mask]\n",
        "    y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "    print(f\"\\n   Training samples: {len(X_train)}\")\n",
        "    print(f\"   Test samples: {len(X_test)}\")\n",
        "\n",
        "    # Train models for each condition\n",
        "    print(\"\\n6. Training calibrated probability models...\")\n",
        "    results = {}\n",
        "\n",
        "    for condition in ['very_hot', 'very_cold', 'very_windy', 'very_wet']:\n",
        "        if condition not in y_train.columns:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n   Training: {condition}\")\n",
        "        model_cond = AuraCastProbabilityModel()\n",
        "        model_cond.train_calibrated_model(X_train, y_train[condition])\n",
        "\n",
        "        # Predict probabilities\n",
        "        y_pred_proba = model_cond.model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Calculate reliability\n",
        "        reliability = model_cond.calculate_reliability_metrics(\n",
        "            y_test[condition].values,\n",
        "            y_pred_proba\n",
        "        )\n",
        "\n",
        "        results[condition] = {\n",
        "            'model': model_cond,\n",
        "            'brier_score': reliability['brier_score'],\n",
        "            'reliability_data': reliability\n",
        "        }\n",
        "\n",
        "        print(f\"      Brier Score: {reliability['brier_score']:.4f}\")\n",
        "        print(f\"      (Lower is better, perfect = 0.0)\")\n",
        "\n",
        "    # Save models\n",
        "    print(\"\\n7. Saving models...\")\n",
        "    joblib.dump(results, 'auracast_probability_models.pkl')\n",
        "    joblib.dump(stats, 'auracast_climatology_stats.pkl')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL TRAINING COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nModels saved:\")\n",
        "    print(\"  - auracast_probability_models.pkl\")\n",
        "    print(\"  - auracast_climatology_stats.pkl\")\n",
        "\n",
        "    # Example prediction\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXAMPLE PREDICTION\")\n",
        "    print(\"=\"*60)\n",
        "    target_date = datetime(2024, 7, 15)\n",
        "    print(f\"\\nQuery: Risk assessment for {target_date.strftime('%B %d, %Y')}\")\n",
        "\n",
        "    # Get last 30 days of features\n",
        "    sample_features = X_test.iloc[-1:].copy()\n",
        "\n",
        "    for condition, result in results.items():\n",
        "        prob = result['model'].model.predict_proba(sample_features)[:, 1][0]\n",
        "        print(f\"\\n{condition.replace('_', ' ').title()}:\")\n",
        "        print(f\"  Probability: {prob*100:.1f}%\")\n",
        "        print(f\"  Model Quality (Brier): {result['brier_score']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqF8O2iSb_Tb",
        "outputId": "4ee7c8ca-6e6a-4930-c4dc-01b2d4de59f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "AURACAST PROBABILITY MODEL TRAINING\n",
            "============================================================\n",
            "\n",
            "1. Fetching NASA POWER data (1990-2023)...\n",
            "2. Processing weather data...\n",
            "   Data points: 12418\n",
            "3. Calculating day-of-year statistics...\n",
            "4. Creating tercile-based targets...\n",
            "5. Engineering predictive features...\n",
            "\n",
            "   Training samples: 10927\n",
            "   Test samples: 1461\n",
            "\n",
            "6. Training calibrated probability models...\n",
            "\n",
            "   Training: very_hot\n",
            "      Brier Score: 0.0244\n",
            "      (Lower is better, perfect = 0.0)\n",
            "\n",
            "   Training: very_cold\n",
            "      Brier Score: 0.0604\n",
            "      (Lower is better, perfect = 0.0)\n",
            "\n",
            "   Training: very_windy\n",
            "      Brier Score: 0.0798\n",
            "      (Lower is better, perfect = 0.0)\n",
            "\n",
            "   Training: very_wet\n",
            "      Brier Score: 0.0404\n",
            "      (Lower is better, perfect = 0.0)\n",
            "\n",
            "7. Saving models...\n",
            "\n",
            "============================================================\n",
            "MODEL TRAINING COMPLETE\n",
            "============================================================\n",
            "\n",
            "Models saved:\n",
            "  - auracast_probability_models.pkl\n",
            "  - auracast_climatology_stats.pkl\n",
            "\n",
            "============================================================\n",
            "EXAMPLE PREDICTION\n",
            "============================================================\n",
            "\n",
            "Query: Risk assessment for July 15, 2024\n",
            "\n",
            "Very Hot:\n",
            "  Probability: 0.0%\n",
            "  Model Quality (Brier): 0.0244\n",
            "\n",
            "Very Cold:\n",
            "  Probability: 0.4%\n",
            "  Model Quality (Brier): 0.0604\n",
            "\n",
            "Very Windy:\n",
            "  Probability: 8.3%\n",
            "  Model Quality (Brier): 0.0798\n",
            "\n",
            "Very Wet:\n",
            "  Probability: 0.0%\n",
            "  Model Quality (Brier): 0.0404\n"
          ]
        }
      ]
    }
  ]
}